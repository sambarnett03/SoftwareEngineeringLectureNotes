## Lecture 6 - Software Testing

- Small errors can accumulate over large projects.

- Automated tests should supplement, but not replace, manual testing.

- Important to start testing from day 1 - you write a function, you write a test, you write a class, you write a test, etc.

### Levels of testing
- Unit tests: test specific units of functionality, ensuring expected outcome given inputs.

- Functional or Integration tests: test functional paths through the code, especially useful for exposing faults in inter-unit interactions.

- Regression tests: ensure unchanged program output despite code modifications. Doesn't test correctness of code, just that it continues to work the same.

- This course will focus on unit tests, with principles applicable to other types of tests

### Pytest
- A popular, robuts unit testing framwork used with Python:
```{python}
def test_reversed():
    assert list(reversed([1, 2, 3])) == [3, 2, 1]
```

- Test functions must start with test_, and be in files called test_.

- Python also has unittest built-in, slightly less popular.

### Debugging
- Unit tests can indicate the location where the code failed, but not more info.

- Lots of methods for debugging - print program state at various points, using logging capabilites for tracking the program's progression (logging is similar to putting print statements everywhere, but means you don't need to remove them), examining intermediate files. If these aren't sufficient, a debugger can provide an in depth exploration of the running code.


### Other useful techniques
- Defensive programming:
    - checks that input data satisfies preconditions before proceeding, e.g checking types and dimensions of inputs to functions
    - raise an error if these conditions are not met

- Automate code style checks:
    - linters analyze source code for errors, inconsistencies, and stylistic issues
    - they promote readability, maintainablility, and adherence to best practices
    - flake8 and black are a linters in python. There are also online services that you attach to your continuous integration e.g codacity

- Code smells are hints of underlying design issues in code, despite it working
    - e.g large classes or methods, methods with too many parameters, duplicated statements in conditional blocks, lost of nested if statements, etc.





## Notes from Exercises
- Workflow for unit testing:
    - put tests in /tests folder
    - call test file test_{whatever file you're testing}
    - call function test_{descriptive name}
    - run python -m pytest tests/filename

### Basic assert statements
- To check equality of native python objects including lists:
```{python}
assert list1 == list2
```


- To check equality of numpy arrays:
```{python}
import numpy.testing as npt
npt.assert_array_equal(array1, array2)
```

- To check equality of numpy arrays containing floats to within 'decimal' amount of decimal places
```{python}
npt.assert_almost_equal(array1, array2, decimal=2)
```

- To check that an error is raised (e.g if checking TypeError is raised when passing string to function expecting integer). The test will pass if the error is raised.
```{python}
import pytest

def test_abc():
    with pytest.raises(ValueError):
        int('Not a number')
```


- Sometimes useful to place a test inside a with pytest.raise and outside it depending on whether an error is raised in the first place.
```{python}
def test_abc():
    if ValueError is not None:          # value error is raised
        with pytest.raises(ValueError):
            npt.assert_array_equal(array1, array2)
    else:
        npt.assert_array_equal(array1, array2)

```


- Sometimes useful to convert a list to a numpy array using the following:
```{python}
if isinstance(test, list):
    test = np.array(test)
```




### Linting
- pylint gives a repo a score out of 10, highlighting issues like unused imports and variables, as well as more complex stylistic issues.

- To use pylint, navigate into the project's root directory and type 'pylint foldername' (after pip install pylint)


### Parameterising Unit Tests
- Paramterisation is a method to pass a unit test many different inputs and outputs to try in a compact way.

- The decorator in the following example tells the function to look for variables 'test' and 'expected', where the variables take on each tuple in the list successively. Notice that 'test' and 'expected' need to be passed to the function as arguments. 
```{python}
import pytest

@pytest.mark.parametrize(
    'test, expected',
    [
        ([[0, 0], [1, 1], [2, 2]], 2),      # (test1, expected1)
        ([[0, 0], [1, 2], [3, 1]], 3)       # (test2, expected2)
    ]
)
def test_abc(test, expected):
    assert complicated_func(test) == expected
```

- You may also want to pass the expected raises that occur:
```{python}
import pytest

@pytest.mark.parametrize(
    'test, expected', 'expected_raises',
    [
        ([[0, 0], [1, 1], [2, 2]], 2, None)     
        ([[0, 0], [1, 2], [3, 1]], 3, None)      
        ([[Nan, -2], [0.5, Nan], [0, 0]], 3, ValueError)
    ]
)
def test_abc(test, expected, expected_raises):
    if expected_raises is not None:
        with pytest.raise(expected_raises):
            complicated_func(test)
    else:
        assert complicated_func(test) == expected

```


### Coverage
- pip install pytest-cov, then run 
```{python}
python -m pytest --cov={filename} tests/{test file name}.py

# e.g
python -m pytest --cov=inflammation.models tests/test_models.py
```

- This gives the percentage of the lines of code that the test file tests. It specifies which lines are not covered by tests



### Testing indeterminate functions
- Often, functions may use some randomness e.g monte carlo simulations.

- To test these, the seed can be set to ensure the same output, or a specific dataset hard coded.

- Alternatively, you can test the data against constraints on it. For example, if the data is randomly generated between 0 and 100, testing that the max value does not exceed 100 and the min value is greater than 0.




### Writing tests inside classes
- When multiple tests are ran on the same piece of data, it is usually better to write the tests as methods of a class. 

- pytest has a special method setup_class(self) which runs before any other methods in the class. The setup_class method can therefore be used to store the data as a class attribute that will be used by the other methods.

- This is particularly useful when testing the methods of a class you wrote. In the following example, the mean, min and max methods that have been written for SomeOtherClass are tested.
```{python}
import pytest

class TestABC:
    def setup_class(self):
        self.instanceofclass1 = SomeOtherClass(id=1, data=[1,2,3,4])
        self.instanceofclass2 = SomeOtherClass(id=2, data=[5,6,7,8])

    def test_1(self):
        assert self.instanceofclass1.mean() == 2.5

    def test_2(self):
        assert self.instanceofclass1.max() == 4

    def test_3(self):
        assert self.instanceofclass1.min() == 1
```

- Note that this cannot be used in conjunction with parameterisation.


### Fixtures within classes
- Features provide an alternate way to provide multiple tests the same input data. 

- Functions are declared outside the class with the decorator @pytest.fixture() which can then be passed as arguments to methods within the class. See the below example for how this works:

```{python}
import pytest

@pytest.fixture()
def instanceofclass1():
    return SomeOtherClass(id=1, data=[1,2,3])

def instanceofclass2():
    return SomeOtherClass(id=2, data=[4,5,6])

class TestABC:
    def test_1(self, instanceofclass1):
        assert self.instanceofclass1.mean() == 2.5

    def test_2(self, instanceofclass1):
        assert self.instanceofclass1.max() == 4

    def test_3(self,instanceofclass1, instanceofclass2):
        assert self.instanceofclass1.min() == 1
        assert self.instanceofclass2.min() == 1
```
- The data created in the fixture is remade everytime the unit test is ran be default. However, the data can be stored for the rest of the session (saving compute time if expensive to calculate) by specifying:
```{python}
@pytest.fixture(scope='session')
```

- Fixtures can be combined with parameterisation to allow several test cases to be run easily. Typically, an instance of the class is made using some arbitrary attributes, which are then overridden inside the test functions with the desired data:
```{python}
import pytest

@pytest.fixture()
def instanceofclass1():
    return SomeOtherClass(id=1, data=[1,2,3])



class TestABC:
    @pytest.mark.parametrize(
        'test, expected',
        [
            ([10, 20.5, 32], 1),
            ([3, 9, 10], 4),
            ([0, 0, 0], 0)
        ]
    )
    def test_1(self, instanceofclass1, test, expected):
        self.instanceofclass1.data = test
        assert self.instanceofclass1.mean() == expected

```


### Fixtures for setup and teardown
- Fixtures can also be used to perform some setup prior tests being ran, and then some clean up task after the tests are ran.

- Rather than using return, fixtures used for setup using the key work yield - all code preceeding yield is executed before the tests, all code afterwards is executed after the tests.

- For example, we might have a setup fixture to connect to a database. A unit test is the run making sure the correct data is retrieved. Once the unit test is completed, the setup fixture closes the connection to the database. While these two tasks could be combined in one unit test, debugging is made easier by separating them. Also, other tests requiring connection to the database can recycle the fixture. 

```{python}
import pytest
import sqlite3
from pathlib import Path
from sqlite_example import connect_to_database, query_database

@pytest.fixture
def setup_database():
    # Connect to database and insert test data before tests run
    conn = sqlite3.connect("test.db")
    cur = conn.cursor()
    cur.execute("CREATE TABLE Animals(Name, Species, Age)")
    cur.execute("INSERT INTO Animals VALUES ('Bugs', 'Rabbit', 6)")
    conn.commit()

    # return object providing connection to database to be used unit test
    yield conn 

    # Teardown database connection after unit tests complete
    cur.execute("DROP TABLE Animals")
    conn.close()
    Path.unlink("test.db")

def test_query_database(setup_database):
    # get connection to database from fixture
    conn = setup_database

    # do some sql

    # test some results

```

- Fixtures can accept other fixtures as arguments. For instance, we could connect to the database in one fixture, and then populate with example data in another. This makes debugging easier:

```{python}
import pytest
import sqlite3
from pathlib import Path
from sqlite_example import connect_to_database, query_database

@pytest.fixture
def database_connection():
    # Create db connection
    db_filename = "test.db"
    conn = sqlite3.connect(db_filename)
    yield conn
    conn.close()
    Path.unlink(db_filename)

@pytest.fixture
def setup_database(database_connection):
    # populate db
    conn = database_connection
    cur = conn.cursor()
    cur.execute("CREATE TABLE Animals(Name, Species, Age)")
    cur.execute("INSERT INTO Animals VALUES ('Bugs', 'Rabbit', 6)")
    conn.commit()
    yield conn
    cur.execute("DROP TABLE Animals")

def test_query_database(setup_database):
    # get connection to database from fixture
    conn = setup_database

    # do some sql

    # test some results
```


- pytest comes with in built fixtures. One such useful feature creates a temporary directory for our files during testing - 'temp_path_factory':
```{python}
import pytest
import sqlite3
from pathlib import Path
from sqlite_example import connect_to_database, query_database


@pytest.fixture(scope="session")
def database_fn_fixture(tmp_path_factory):
    # create temp folder and file at data/test.db and pass path to next fixture
    yield tmp_path_factory.mktemp("data") / "test.db"

@pytest.fixture(scope="session")
def database_connection(database_fn_fixture):
    # take pathname to temp file and connect
    conn = sqlite3.connect(database_fn_fixture)
    yield conn
    conn.close()
    Path.unlink(database_fn_fixture)
```




### Mocking
- Mocking is when real parts of your system are replaced with mock objects that simulate the behaviour of the real object.

- This is useful when:
    - the real object is slow (e.g databse or api call)
    - the real object is hard to control (e.g returns random data)
    - the real object has side effects (e.g send an email or writes to disk)

- To give an example, imagine you have a function get_user_profile(api_client, user_id), which returns the users name in all caps and whether they are an adult:
```{python}
def get_user_profile(api_client, user_id):
    data = api_client.fetch_user(user_id)
    return {
        "name": data["name"].upper(),
        "is_adult": data["age"] >= 18
    }

```

- We want to test this functionality. However, we don't want to actually make the api call with api_client.fetch_user() in our test. Instead, we make a Mock object that returns an example of what api_client.fetch_user() would return. This allows us to test that get_user_profile() is transforming the data correctly.

```{python}
from unittest.mock import Mock
from app import get_user_profile

def test_get_user_profile():
    mock_api_client = Mock()
    mock_api_client.fetch_user.return_value = {"name": "alice", "age": 20}

    result = get_user_profile(mock_api_client, 123)

    assert result == {"name": "ALICE", "is_adult": True}
    mock_api_client.fetch_user.assert_called_once_with(123)

```

- The mock_api_client.fetch_user.assert_called_once_with(123) ensures the code called the api_client.fetch_user only once. This is especially useful in cases where the code is more complicated - in this case it's quite clear the it's only be called once. If get_user_profile accidently made two calls to the API, the test would now fail:
```{python}
def get_user_profile(api_client, user_id):
    api_client.fetch_user(user_id)
    data = api_client.fetch_user(user_id)
    return {"name": data["name"], "age": data["age"]}

```

- Mocks can also be used to simulate errors and ensure they are handled correctly. For example, you could set the mock_api_client to raise a network error exception. You can then make sure that get_user_profile handles the raised error correctly.
```{python}
mock_api_client.fetch_user.side_effect = Exception("Network error")
```

- The methods of mocks are also mocks. The most useful things to do are assign callable behaviours with .return_value and .side_effect, as well as checking how many times the mock's method was called
```{python}
factory = Mock()
factory.return_value = 2
factory.side_effect = Exception('Boo')

factory.mymethod.return_value = [2, 2, 2]

assert factory.mymethod.assert_called_once()
```


### Patches
- The only downside to Mocks is that they are very manual - you're prone to making an error, and they're hard to clean up when you're done with it.

- unittest.mock.path automatically makes a replacement with a mock, and is scoped to the function. So for example, if you wanted to replace a database connection with a mock for a certain period of time, you could use a patch.

- There are two main ways to use patch:
    - as a decorator
    - in a with block

- Conceptually, the with makes more sense so we'll start with that

```{python}
import pytest
import sqlite3
import query_database   # function to test
from unittest.mock import patch

def test_query_database():
    with patch('sqlite3.connect') as mock_connection:
        # make mock connection
        conn = mock_connection('abc')

        # add things to database and call function for testing
        result = query_database()

        # do tests and close connection
        conn.close()

```

- There's some advantage to patching that I don't understand, but basically if mocking fails, patching is probably the way to go due to scoping advantages and module access.

- The above code works the same with the decorator @patch('sqlite3.connect') and the function definition def test_query_db_mocked_connection(mock_connection).

