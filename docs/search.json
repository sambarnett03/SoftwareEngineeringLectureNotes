[
  {
    "objectID": "16-10-25pm.html",
    "href": "16-10-25pm.html",
    "title": "My Lecture Notes",
    "section": "",
    "text": "Use Github Issues for tracking tasks\n\ncan define template for issues\ncan add labels to issues such as bug, feature, priority: high etc.\nfor bugs typically want steps to reproduce\ncan assign people to issues\ncan add comments\ncan make a new branch to solve current issues, maybe include issue number in branch name\n\nHave main branch that holds tested, working code. Issues are raised with this code. New branches are created to solve these issues. Once solved, the branch created for the issue is merged with main. The old branch can then be deleted.\nThe merging of issue branches and main is not trivial. Will cover in detail later.\n\n\n\n\n\nOrganise how to let people write to your repo\nTwo main options:\n\nshared repo model - common in small, private projects. Every developer has permission to write to the repo.\nFork and pull model - popular in large, open source projects. Have original repo that one person has access to, or small group of people. Everyone else who wants to work on it forks the repo into the personal repo, does their work, then pull requests back to original repo (requiring the approval of the owner / small group in charge). Forking the repo produces a copy of it that they have access to (original repo referred to as origin, forked repo as fork)\n\n\n\n\n\n\nUsed to merge a branch or fork. Can create templates for pull request page including e.g important checks.\nGithub supports automated tests to be set up for pull requests. They are .yml files that contain instructions such as checking linting, style and unit tests. These tests run on Github ‘runners’.\nTypically then want someone else working on the project to check the code. You can add comments to the code and give feedback.",
    "crumbs": [
      "Home",
      "Lecture 5: Collaborative Code Development with Git and Github"
    ]
  },
  {
    "objectID": "16-10-25pm.html#lecture-5-collaborative-code-development-with-git-and-github",
    "href": "16-10-25pm.html#lecture-5-collaborative-code-development-with-git-and-github",
    "title": "My Lecture Notes",
    "section": "",
    "text": "Use Github Issues for tracking tasks\n\ncan define template for issues\ncan add labels to issues such as bug, feature, priority: high etc.\nfor bugs typically want steps to reproduce\ncan assign people to issues\ncan add comments\ncan make a new branch to solve current issues, maybe include issue number in branch name\n\nHave main branch that holds tested, working code. Issues are raised with this code. New branches are created to solve these issues. Once solved, the branch created for the issue is merged with main. The old branch can then be deleted.\nThe merging of issue branches and main is not trivial. Will cover in detail later.\n\n\n\n\n\nOrganise how to let people write to your repo\nTwo main options:\n\nshared repo model - common in small, private projects. Every developer has permission to write to the repo.\nFork and pull model - popular in large, open source projects. Have original repo that one person has access to, or small group of people. Everyone else who wants to work on it forks the repo into the personal repo, does their work, then pull requests back to original repo (requiring the approval of the owner / small group in charge). Forking the repo produces a copy of it that they have access to (original repo referred to as origin, forked repo as fork)\n\n\n\n\n\n\nUsed to merge a branch or fork. Can create templates for pull request page including e.g important checks.\nGithub supports automated tests to be set up for pull requests. They are .yml files that contain instructions such as checking linting, style and unit tests. These tests run on Github ‘runners’.\nTypically then want someone else working on the project to check the code. You can add comments to the code and give feedback.",
    "crumbs": [
      "Home",
      "Lecture 5: Collaborative Code Development with Git and Github"
    ]
  },
  {
    "objectID": "14-10-25-am.html",
    "href": "14-10-25-am.html",
    "title": "Lecture 2: Object Oriented Programming (oop)",
    "section": "",
    "text": "Plan at start of project how to architect the code - this way can allow for most flexibility in code\n\ntypes of things we need to represent\nwhat data do we need\nwhat are the relationships between the types of data\n\n\n\nExample: limits of functional programming\n\ntemp = 0\nsteps = 1e8\n\nfor step in range(steps):\n    gas = initialise_gas(temp)\n    photons = generate_photons()\n    temp += calculate_temperature(gas, photons)\n\nprint('temperature is', temp)\n\nNow extend to new use case.\n\ntemp = 0\nsteps = 1e8\nphoton_source = 'black hole'\n\nfor step in range(steps):\n    gas = initialise_gas(temp)\n    if photon_source = 'black hole':\n        photons = generate_photons_black_hole()\n    else:\n        photons = generate_photons()\n    temp += calculate_temperature(gas, photons)\n\nprint('temperature is', temp)\n\nNow not ideal, looping through unecessarily, code is also more messy, especially if you add another source of photons. Or, if you want to add another temperature model. Then more if statements than before.\nOop maybe better in this case\n\n\nExample: intro to oop\n\nacademics = []\npapers = []\n\ndef write_paper(academics, papers, academic, paper):\n    academics.append(academic)\n    papers.append(paper)\n\nwrite_paper(academics, papers, 'NAME1', 'NAME2')\n\nMain issue here is that we have two separate bits of data, not clear how they relate, and the relationship between the function and data is a little abstract and tenuous. Oop adds meaning to data. Associate the functions with the data, so the data lives inside the function class.\nA class defines a bundle of data and functions. An object is an instance of a class. All objects have the same behaviour (method), but different data (attributes).\n\nclass Academic:\n    def __init__(self, name):\n        self.name = name\n        self.papers = []\n\n    def write_paper(self, title):\n        self.papers.append(title)\n\nsam = Academic('Name1')\nsam.write_paper('Paper1')\n\nThis is much cleaner, less passing of data around.\n\n\nEntity diagrams\nDraw a diagram - Consider the objects in the problem - What do they do? - What properties do they have? - How do they relate?\n\n\nInheritance\n\nidentity relationship (‘x is a y’)\ncan go down multiple steps\neach has access to all functions of the parent\n\ne.g person has name and office admin is subclass of person academic is subclass of person with additonal attributes papers student is subclass of academic with additional attribute graduate lecturer is subclass of academic with additional attribute teach course\n\n\nComposition\n\nOwnership relationship (‘x has a y’)\nClasses can have objects as data within them\n\ne.g define class called paper with title and text store list of these papers within academic class this is a composition relationship\nComposition and inheritance can be combined. There is usually many ways to acheive the same thing. Use the simplest and clearest solution where possible.\nSometimes using inheritance a lot can be cumbersome. You can end up bringing in functionality to a class from its parent class that is not needed. This is especially common when using many classes with lots of layers. Composition can negate this, only bringing in the methods that you want. In general, use composition where possible.\n\n\nPolymorphism\n\nCan mean modified inheritance\nCan mean different implementations of the same method\n\ne.g academic has papers and write papers phd student is a subclass of academic research software engineer writes slightly different papers, so can still make as a subclass of academic but override the write paper method\nThis is useful as the user jkust calls .write_paper, but this does a slightly different things depending on the subclass its called from. This allows them both to be treated as a generic list of academics. This means the user doesn’t need to care about the difference between phd student and research software engineer.\nIn this case, the academic class may never actually be used, only its subclasses. In this case, academic is called a virtual class - it is never used itself. Libraries exist allowing you to make a class virtual - i.e it stops it being constructed.\nYou can also have virtual functions - write paper in academic could be overidden by both phd student and software engineer in which case the original write_paper would never be called.\nIn entity diagrams, virtual classes and functions are represented with dashed lines.\n\n\nExample: oop redesign for temp calculation\nMake a class called temp_model with virtual function calculate temp. subclass this into two different temp models, e.g matthews model and sims model - write the calc_temp function for each.\nSimilarly, one class called photon source with method generate. subclass this into black hole and white dwarf, with generate function for each.\nFinal class called gas cloud. Compose temp model into gas cloud with methods temp model (through composition), calc_temp, calc_structure, and attributes temp and structure.\nNow code looks like:\n\nsteps = 1e8\nphoton_source = PhotonSourceBlackHole()\ngas_cloud = GasCloud(\n    temp_model = TemperatureModelMatthews()     # this is composition\n    start_temp = 0\n)\n\nfor step in range(steps):\n    gas_cloud = cal_structure()\n    photons = photon_source.generate()\n    gas_cloud.cal_temp(photons)\n\nprint('final temp is', temp)\n\n\n\nNote on Python\nIn python, isinstance() considers inheritance, so an instant of a subclass will pass when compared with its parent class. type() does not account for subclasses.\n\n\nNote on static methods\nStatic methods are used when you have a function that is logically bound to class but doesn’t require access to the class itself. For example, if you have a Date() class with attributes day, month year, you may want to validate whether a data as presented as a string is valid by checking if the first number &lt; 31, the second &lt; 12 etc. A function to do this can be added to the Date class as a static method, and is then called using Data.is_date_valid(). In this way it becomes associated with the class, without having any access to the internals.\ne.g:\n\nclass Date(object):\n    \n    def __init__(self, day=0, month=0, year=0):\n        self.day = day\n        self.month = month\n        self.year = year\n\n    @staticmethod\n    def is_date_valid(date_as_string):\n        day, month, year = map(int, date_as_string.split('-'))\n        return day &lt;= 31 and month &lt;= 12 and year &lt;= 3999\n\n# usage:\nis_date = Date.is_date_valid('11-09-2012')\n\n\n\nClass methods\nClass methods are bound to the class itself, not an instance of the class. This means they can be used as alternative way of instanteating a class. In the above example, an instance of the Data class can be created using three integers (day, month and year). However, what if you wanted to create an instance of the class from a string in ddmmyyyy format. Adding a class method to the Date class allows this.\ne.g\n\nclass Date(object):\n    \n    def __init__(self, day=0, month=0, year=0):\n        self.day = day\n        self.month = month\n        self.year = year\n\n    @classmethod\n    def from_string(cls, date_as_string):\n        day, month, year = map(int, date_as_string.split('-'))\n        date1 = cls(day, month, year)\n        return date1\n\ndate2 = Date.from_string('11-09-2012')\n\ndate2 is now an instance of the Date class, generated from a string. You could in principle have a separate function in the code that parses a string into three integers, and then use this to instantiate the Date object, but this is less clean than using a class method\nClass methods are also used to edit class level data. If a class has an attribute such as company_name = ‘ABC’ defined OUTSIDE the init (making it belong to the class not the instance), this can be edited using a class method.\ne.g\n\nclass Employee:\n    company_name = \"TechCorp\"\n\n    def __init__(self, name):\n        self.name = name\n        Employee.employee_count += 1\n\n    @classmethod\n    def set_company_name(cls, name):\n        cls.company_name = name\n\nA final point on class methods is that they support inheritance. The first argument, cls, refers to the class that called the method - not necessarily the where it was defined.\ne.g\n\nclass Date:\n    def __init__(self, day, month, year):\n        self.day = day\n        self.month = month\n        self.year = year\n\n    @classmethod\n    def from_string(cls, date_as_string):\n        day, month, year = map(int, date_as_string.split('-'))\n        return cls(day, month, year)\n\n\nclass DateTime(Date):\n    def __init__(self, day, month, year, hour=0, minute=0):\n        super().__init__(day, month, year)\n        self.hour = hour\n        self.minute = minute\n\n\ndt = DateTime.from_string(\"14-10-2025\")\n\nSo here, from_string is created inside the base class Date(). When .from_string is called from a Date object, the class referenced (cls) is a Data object, so Date.from_string(‘abc’) returns a Date(a, b, c). However, when .from_string is called on a DateTime object, e.g DateTime.from_string(‘abc’), the class method returns DateTime(a, b, c), hence returning a DateTime like object. Note that the class method itself was inherited by DateTime from Date, which is why it can be called at all.\nUsing the same example but with a static method would look like:\n\n@staticmethod\ndef from_string(date_as_string):\n    day, month, year = map(int, date_as_string.split('-'))\n    return Date(day, month, year)  \n\nHere, the method does not take cls as an argument, and will only return a Date type object. This is less flexible",
    "crumbs": [
      "Home",
      "Lecture 2: Object Oriented Programming (oop)"
    ]
  },
  {
    "objectID": "20-10-25am.html",
    "href": "20-10-25am.html",
    "title": "Lecture 7: Continuous Integration( CI)",
    "section": "",
    "text": "An automated process for verifying and integrating code changes.\nObjective: to detect and resolve issues early by frequently testing and integrating code changes. Cognitive overhead is much smaller by solving problems early while you still remember everything.\nEnsure compatability between mac, linux and windows.\nAvoiding dependencies on specific user data or development machine configurations e.g different versions of python, or hard code directory path for specific file you’re using.",
    "crumbs": [
      "Home",
      "Lecture 7: Continuous Integration( CI)"
    ]
  },
  {
    "objectID": "20-10-25am.html#notes-from-exercises",
    "href": "20-10-25am.html#notes-from-exercises",
    "title": "Lecture 7: Continuous Integration( CI)",
    "section": "Notes from exercises",
    "text": "Notes from exercises\n\nTo create a workflow, make a folder at\n\n.github/workflows   # in your root project directory\n\nWorkflows are made as .yml files in this folder. e.g:\n\nname: Basic GitHub Actions Workflow\n\non:\n  push:\n  pull_request:\n  workflow_dispatch:\n\njobs:\n  basic-job:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Run a one-line script\n        run: echo \"Hello, world!\"\n\nBreaking down this example:\n\non: block describes when the workflow should be run. An empty argument implies it should run on every instance of the case e.g every push. Additional arguments are e.g specific branch names. workflow-dispatch gives the option to run the test with the GUI\njobs: block gives the actual tests that will run.\nthe name argument in steps helps to quickly recognise which step failed.\n\nBasic templates can be searched for on GitHub. An example for running Python on various versions is below:\n\n\nname: Python package\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n  workflow_dispatch:\n\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip setuptools wheel\n        python -m pip install .[dev]\n    - name: Lint with flake8\n      run: |\n        # stop the build if there are Python syntax errors or undefined names\n        flake8\n    - name: Test with pytest\n      run: |\n        pytest\n\nCode breakdown:\n\nfail-fast: if one version fails, the rest will continue to run\nmatrix: allows several versions to be ran at once. Could replace with os-version: [ubuntu-latest, macos-latest, windows-latest] to access os versions.\nThe rest is kind of boiler plate - just copy and paste. The tests would typically be more complex than just pytest, see example later.\n\n\n\nTest coverage\n\nThe extent to which the tests cover each line in your code can be found using pytest –cov. An example is:\n\npytest --cov-config=.coveragerc --cov=./ci_course --cov-report=xml\n- --cov-config=.coveragerc specific the config file for the coverage assessment. The autogenerated one simply has ignore files in the test folder.\n- --cov-./ci_course gives the folder to check the coverage of.\n- --cov-report.xml gives the type of output file. \n\nGithub actions can automate this by placing it inside a worfklow, which will be shown below.\nCodecov is website that allows you to easily view the output of such a coverage test, for all your github repos. To set up codecov:\n\nsign in with github account\nclick configure next to your repository\nuse githubactions, pytest, and select repository token.\nnavigate to repo in github, go to settings, secrets and variables, actions, new repository secret, then put ‘CODECOV_TOKEN’ as the name and the generated string from code_cov as the variable.\n\nTo automate the testing of coverage of a repo, add this to workflow\n\n# This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python\n\nname: Coverage Report\n- name: Run coverage test\n    run: |\n    pytest --cov-config=.coveragerc --cov=./ci_course --cov-report=xml --cov-branch\n\n- name: Upload coverage reports to Codecov\n    uses: codecov/codecov-action@v5\n    with:\n    token: ${{ secrets.CODECOV_TOKEN }}\n    fail_ci_if_error: true\n    files: coverage.xml\n\nCodecov should now show the coverage - you can view it line by line.\n\n\n\nMaking Documentation\n\nWe use sphinx and read the docs.\nInside a venv, make a docs folder in the root of your project and type:\n\npip install sphinx\nsphinx-quickstart\n\nAdd a .gitkeep file inside the _static folder so it gets pushed correctly.\nGo to Read the Docs.\n\nlogin with github\nimport a project from your github\nfollow the instructions, leaving everything as default\nthis should build the website from the docs folder in the repo",
    "crumbs": [
      "Home",
      "Lecture 7: Continuous Integration( CI)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Software Engineering Lecture Notes",
    "section": "",
    "text": "Course Overview\nThese notes accompany the lectures for Software Engeering."
  },
  {
    "objectID": "17-10-25am.html",
    "href": "17-10-25am.html",
    "title": "Lecture 6 - Software Testing",
    "section": "",
    "text": "Small errors can accumulate over large projects.\nAutomated tests should supplement, but not replace, manual testing.\nImportant to start testing from day 1 - you write a function, you write a test, you write a class, you write a test, etc.",
    "crumbs": [
      "Home",
      "Lecture 6 - Software Testing"
    ]
  },
  {
    "objectID": "17-10-25am.html#notes-from-exercises",
    "href": "17-10-25am.html#notes-from-exercises",
    "title": "Lecture 6 - Software Testing",
    "section": "Notes from Exercises",
    "text": "Notes from Exercises\n\nWorkflow for unit testing:\n\nput tests in /tests folder\ncall test file test_{whatever file you’re testing}\ncall function test_{descriptive name}\nrun python -m pytest tests/filename\n\n\n\nBasic assert statements\n\nTo check equality of native python objects including lists:\n\n\nassert list1 == list2\n\n\nTo check equality of numpy arrays:\n\n\nimport numpy.testing as npt\nnpt.assert_array_equal(array1, array2)\n\n\nTo check equality of numpy arrays containing floats to within ‘decimal’ amount of decimal places\n\n\nnpt.assert_almost_equal(array1, array2, decimal=2)\n\n\nTo check that an error is raised (e.g if checking TypeError is raised when passing string to function expecting integer). The test will pass if the error is raised.\n\n\nimport pytest\n\ndef test_abc():\n    with pytest.raises(ValueError):\n        int('Not a number')\n\n\nSometimes useful to place a test inside a with pytest.raise and outside it depending on whether an error is raised in the first place.\n\n\ndef test_abc():\n    if ValueError is not None:          # value error is raised\n        with pytest.raises(ValueError):\n            npt.assert_array_equal(array1, array2)\n    else:\n        npt.assert_array_equal(array1, array2)\n\n\nSometimes useful to convert a list to a numpy array using the following:\n\n\nif isinstance(test, list):\n    test = np.array(test)\n\n\n\nLinting\n\npylint gives a repo a score out of 10, highlighting issues like unused imports and variables, as well as more complex stylistic issues.\nTo use pylint, navigate into the project’s root directory and type ‘pylint foldername’ (after pip install pylint)\n\n\n\nParameterising Unit Tests\n\nParamterisation is a method to pass a unit test many different inputs and outputs to try in a compact way.\nThe decorator in the following example tells the function to look for variables ‘test’ and ‘expected’, where the variables take on each tuple in the list successively. Notice that ‘test’ and ‘expected’ need to be passed to the function as arguments.\n\n\nimport pytest\n\n@pytest.mark.parametrize(\n    'test, expected',\n    [\n        ([[0, 0], [1, 1], [2, 2]], 2),      # (test1, expected1)\n        ([[0, 0], [1, 2], [3, 1]], 3)       # (test2, expected2)\n    ]\n)\ndef test_abc(test, expected):\n    assert complicated_func(test) == expected\n\n\nYou may also want to pass the expected raises that occur:\n\n\nimport pytest\n\n@pytest.mark.parametrize(\n    'test, expected', 'expected_raises',\n    [\n        ([[0, 0], [1, 1], [2, 2]], 2, None)     \n        ([[0, 0], [1, 2], [3, 1]], 3, None)      \n        ([[Nan, -2], [0.5, Nan], [0, 0]], 3, ValueError)\n    ]\n)\ndef test_abc(test, expected, expected_raises):\n    if expected_raises is not None:\n        with pytest.raise(expected_raises):\n            complicated_func(test)\n    else:\n        assert complicated_func(test) == expected\n\n\n\nCoverage\n\npip install pytest-cov, then run\n\n\npython -m pytest --cov={filename} tests/{test file name}.py\n\n# e.g\npython -m pytest --cov=inflammation.models tests/test_models.py\n\n\nThis gives the percentage of the lines of code that the test file tests. It specifies which lines are not covered by tests\n\n\n\nTesting indeterminate functions\n\nOften, functions may use some randomness e.g monte carlo simulations.\nTo test these, the seed can be set to ensure the same output, or a specific dataset hard coded.\nAlternatively, you can test the data against constraints on it. For example, if the data is randomly generated between 0 and 100, testing that the max value does not exceed 100 and the min value is greater than 0.\n\n\n\nWriting tests inside classes\n\nWhen multiple tests are ran on the same piece of data, it is usually better to write the tests as methods of a class.\npytest has a special method setup_class(self) which runs before any other methods in the class. The setup_class method can therefore be used to store the data as a class attribute that will be used by the other methods.\nThis is particularly useful when testing the methods of a class you wrote. In the following example, the mean, min and max methods that have been written for SomeOtherClass are tested.\n\n\nimport pytest\n\nclass TestABC:\n    def setup_class(self):\n        self.instanceofclass1 = SomeOtherClass(id=1, data=[1,2,3,4])\n        self.instanceofclass2 = SomeOtherClass(id=2, data=[5,6,7,8])\n\n    def test_1(self):\n        assert self.instanceofclass1.mean() == 2.5\n\n    def test_2(self):\n        assert self.instanceofclass1.max() == 4\n\n    def test_3(self):\n        assert self.instanceofclass1.min() == 1\n\n\nNote that this cannot be used in conjunction with parameterisation.\n\n\n\nFixtures within classes\n\nFeatures provide an alternate way to provide multiple tests the same input data.\nFunctions are declared outside the class with the decorator @pytest.fixture() which can then be passed as arguments to methods within the class. See the below example for how this works:\n\n\nimport pytest\n\n@pytest.fixture()\ndef instanceofclass1():\n    return SomeOtherClass(id=1, data=[1,2,3])\n\ndef instanceofclass2():\n    return SomeOtherClass(id=2, data=[4,5,6])\n\nclass TestABC:\n    def test_1(self, instanceofclass1):\n        assert self.instanceofclass1.mean() == 2.5\n\n    def test_2(self, instanceofclass1):\n        assert self.instanceofclass1.max() == 4\n\n    def test_3(self,instanceofclass1, instanceofclass2):\n        assert self.instanceofclass1.min() == 1\n        assert self.instanceofclass2.min() == 1\n\n\nThe data created in the fixture is remade everytime the unit test is ran be default. However, the data can be stored for the rest of the session (saving compute time if expensive to calculate) by specifying:\n\n\n@pytest.fixture(scope='session')\n\n\nFixtures can be combined with parameterisation to allow several test cases to be run easily. Typically, an instance of the class is made using some arbitrary attributes, which are then overridden inside the test functions with the desired data:\n\n\nimport pytest\n\n@pytest.fixture()\ndef instanceofclass1():\n    return SomeOtherClass(id=1, data=[1,2,3])\n\n\n\nclass TestABC:\n    @pytest.mark.parametrize(\n        'test, expected',\n        [\n            ([10, 20.5, 32], 1),\n            ([3, 9, 10], 4),\n            ([0, 0, 0], 0)\n        ]\n    )\n    def test_1(self, instanceofclass1, test, expected):\n        self.instanceofclass1.data = test\n        assert self.instanceofclass1.mean() == expected\n\n\n\nFixtures for setup and teardown\n\nFixtures can also be used to perform some setup prior tests being ran, and then some clean up task after the tests are ran.\nRather than using return, fixtures used for setup using the key work yield - all code preceeding yield is executed before the tests, all code afterwards is executed after the tests.\nFor example, we might have a setup fixture to connect to a database. A unit test is the run making sure the correct data is retrieved. Once the unit test is completed, the setup fixture closes the connection to the database. While these two tasks could be combined in one unit test, debugging is made easier by separating them. Also, other tests requiring connection to the database can recycle the fixture.\n\n\nimport pytest\nimport sqlite3\nfrom pathlib import Path\nfrom sqlite_example import connect_to_database, query_database\n\n@pytest.fixture\ndef setup_database():\n    # Connect to database and insert test data before tests run\n    conn = sqlite3.connect(\"test.db\")\n    cur = conn.cursor()\n    cur.execute(\"CREATE TABLE Animals(Name, Species, Age)\")\n    cur.execute(\"INSERT INTO Animals VALUES ('Bugs', 'Rabbit', 6)\")\n    conn.commit()\n\n    # return object providing connection to database to be used unit test\n    yield conn \n\n    # Teardown database connection after unit tests complete\n    cur.execute(\"DROP TABLE Animals\")\n    conn.close()\n    Path.unlink(\"test.db\")\n\ndef test_query_database(setup_database):\n    # get connection to database from fixture\n    conn = setup_database\n\n    # do some sql\n\n    # test some results\n\n\nFixtures can accept other fixtures as arguments. For instance, we could connect to the database in one fixture, and then populate with example data in another. This makes debugging easier:\n\n\nimport pytest\nimport sqlite3\nfrom pathlib import Path\nfrom sqlite_example import connect_to_database, query_database\n\n@pytest.fixture\ndef database_connection():\n    # Create db connection\n    db_filename = \"test.db\"\n    conn = sqlite3.connect(db_filename)\n    yield conn\n    conn.close()\n    Path.unlink(db_filename)\n\n@pytest.fixture\ndef setup_database(database_connection):\n    # populate db\n    conn = database_connection\n    cur = conn.cursor()\n    cur.execute(\"CREATE TABLE Animals(Name, Species, Age)\")\n    cur.execute(\"INSERT INTO Animals VALUES ('Bugs', 'Rabbit', 6)\")\n    conn.commit()\n    yield conn\n    cur.execute(\"DROP TABLE Animals\")\n\ndef test_query_database(setup_database):\n    # get connection to database from fixture\n    conn = setup_database\n\n    # do some sql\n\n    # test some results\n\n\npytest comes with in built fixtures. One such useful feature creates a temporary directory for our files during testing - ‘temp_path_factory’:\n\n\nimport pytest\nimport sqlite3\nfrom pathlib import Path\nfrom sqlite_example import connect_to_database, query_database\n\n\n@pytest.fixture(scope=\"session\")\ndef database_fn_fixture(tmp_path_factory):\n    # create temp folder and file at data/test.db and pass path to next fixture\n    yield tmp_path_factory.mktemp(\"data\") / \"test.db\"\n\n@pytest.fixture(scope=\"session\")\ndef database_connection(database_fn_fixture):\n    # take pathname to temp file and connect\n    conn = sqlite3.connect(database_fn_fixture)\n    yield conn\n    conn.close()\n    Path.unlink(database_fn_fixture)\n\n\n\nMocking\n\nMocking is when real parts of your system are replaced with mock objects that simulate the behaviour of the real object.\nThis is useful when:\n\nthe real object is slow (e.g databse or api call)\nthe real object is hard to control (e.g returns random data)\nthe real object has side effects (e.g send an email or writes to disk)\n\nTo give an example, imagine you have a function get_user_profile(api_client, user_id), which returns the users name in all caps and whether they are an adult:\n\n\ndef get_user_profile(api_client, user_id):\n    data = api_client.fetch_user(user_id)\n    return {\n        \"name\": data[\"name\"].upper(),\n        \"is_adult\": data[\"age\"] &gt;= 18\n    }\n\n\nWe want to test this functionality. However, we don’t want to actually make the api call with api_client.fetch_user() in our test. Instead, we make a Mock object that returns an example of what api_client.fetch_user() would return. This allows us to test that get_user_profile() is transforming the data correctly.\n\n\nfrom unittest.mock import Mock\nfrom app import get_user_profile\n\ndef test_get_user_profile():\n    mock_api_client = Mock()\n    mock_api_client.fetch_user.return_value = {\"name\": \"alice\", \"age\": 20}\n\n    result = get_user_profile(mock_api_client, 123)\n\n    assert result == {\"name\": \"ALICE\", \"is_adult\": True}\n    mock_api_client.fetch_user.assert_called_once_with(123)\n\n\nThe mock_api_client.fetch_user.assert_called_once_with(123) ensures the code called the api_client.fetch_user only once. This is especially useful in cases where the code is more complicated - in this case it’s quite clear the it’s only be called once. If get_user_profile accidently made two calls to the API, the test would now fail:\n\n\ndef get_user_profile(api_client, user_id):\n    api_client.fetch_user(user_id)\n    data = api_client.fetch_user(user_id)\n    return {\"name\": data[\"name\"], \"age\": data[\"age\"]}\n\n\nMocks can also be used to simulate errors and ensure they are handled correctly. For example, you could set the mock_api_client to raise a network error exception. You can then make sure that get_user_profile handles the raised error correctly.\n\n\nmock_api_client.fetch_user.side_effect = Exception(\"Network error\")\n\n\nThe methods of mocks are also mocks. The most useful things to do are assign callable behaviours with .return_value and .side_effect, as well as checking how many times the mock’s method was called\n\n\nfactory = Mock()\nfactory.return_value = 2\nfactory.side_effect = Exception('Boo')\n\nfactory.mymethod.return_value = [2, 2, 2]\n\nassert factory.mymethod.assert_called_once()\n\n\n\nPatches\n\nThe only downside to Mocks is that they are very manual - you’re prone to making an error, and they’re hard to clean up when you’re done with it.\nunittest.mock.path automatically makes a replacement with a mock, and is scoped to the function. So for example, if you wanted to replace a database connection with a mock for a certain period of time, you could use a patch.\nThere are two main ways to use patch:\n\nas a decorator\nin a with block\n\nConceptually, the with makes more sense so we’ll start with that\n\n\nimport pytest\nimport sqlite3\nimport query_database   # function to test\nfrom unittest.mock import patch\n\ndef test_query_database():\n    with patch('sqlite3.connect') as mock_connection:\n        # make mock connection\n        conn = mock_connection('abc')\n\n        # add things to database and call function for testing\n        result = query_database()\n\n        # do tests and close connection\n        conn.close()\n\n\nThere’s some advantage to patching that I don’t understand, but basically if mocking fails, patching is probably the way to go due to scoping advantages and module access.\nThe above code works the same with the decorator @patch(‘sqlite3.connect’) and the function definition def test_query_db_mocked_connection(mock_connection).",
    "crumbs": [
      "Home",
      "Lecture 6 - Software Testing"
    ]
  },
  {
    "objectID": "13-10-25.html",
    "href": "13-10-25.html",
    "title": "My Lecture Notes",
    "section": "",
    "text": "Software group do courses on CUDA and cloud computing, useful to ask if I need help: rse.ox.ac.uk, can sign onto courses.\nSoftware is important for reproducibility\nDifference between programming and engineering: programming is algorithms, syntax, individual, one line at a time. Engineering considers the life cycle of the software, maintenance, refactoring, considering stakeholders, building for the future etc.\nWaterfall model:\n\nrequirement analysis\ndesign\nimplementation\ntesting\nrelease\nmaintenance not so useful in academia as too fixed, agile model turns this into a loop and attempts to make the minimum viable product, the requirements are always assessed and reconsidered.\n\nRecommended book: facts and fallacies of software engineering\nAll training on https://train.rse.ox.ac.uk/\nNotes for myself:\n\ntry to include unit testing\nuse WSl from day 1\nhow to include pseudo code in quarto\nlearn about testing during programme",
    "crumbs": [
      "Home",
      "Lecture 1: Introduction"
    ]
  },
  {
    "objectID": "13-10-25.html#lecture-1-introduction",
    "href": "13-10-25.html#lecture-1-introduction",
    "title": "My Lecture Notes",
    "section": "",
    "text": "Software group do courses on CUDA and cloud computing, useful to ask if I need help: rse.ox.ac.uk, can sign onto courses.\nSoftware is important for reproducibility\nDifference between programming and engineering: programming is algorithms, syntax, individual, one line at a time. Engineering considers the life cycle of the software, maintenance, refactoring, considering stakeholders, building for the future etc.\nWaterfall model:\n\nrequirement analysis\ndesign\nimplementation\ntesting\nrelease\nmaintenance not so useful in academia as too fixed, agile model turns this into a loop and attempts to make the minimum viable product, the requirements are always assessed and reconsidered.\n\nRecommended book: facts and fallacies of software engineering\nAll training on https://train.rse.ox.ac.uk/\nNotes for myself:\n\ntry to include unit testing\nuse WSl from day 1\nhow to include pseudo code in quarto\nlearn about testing during programme",
    "crumbs": [
      "Home",
      "Lecture 1: Introduction"
    ]
  },
  {
    "objectID": "13-10-25.html#virtual-environments",
    "href": "13-10-25.html#virtual-environments",
    "title": "My Lecture Notes",
    "section": "Virtual Environments",
    "text": "Virtual Environments\nTo create a virtual environment, type\npython3 -m venv {desired name of venv}\nsource {desired name of venv}/bin/activate\nThe first line of code creates the virtual environment. It makes a folder in the cwd that contains all the info and libraries regarding the virtual environment. The second line of code actually activates the virtual environment.\nTo deactivate the virtual environment, type:\ndeactivate\nTo delete a virtual environment, move into the directory containing its folder and type\nrm -r {name of venv}\n\nTesting style for pseudo code\n\n\nPseudocode\n\ncat vcf_file | sed -n '/#CHROM/,$p' | cut -f 1,2,4,5 &gt; variant_info.txt",
    "crumbs": [
      "Home",
      "Lecture 1: Introduction"
    ]
  },
  {
    "objectID": "20-10-25pm.html",
    "href": "20-10-25pm.html",
    "title": "Lecture 8: Packaging and Dependency Management",
    "section": "",
    "text": "Packages are a bundle of code, resources and metadata (e.g version of python needed etc), e.g numpy, pandas etc.",
    "crumbs": [
      "Home",
      "Lecture 8: Packaging and Dependency Management"
    ]
  },
  {
    "objectID": "20-10-25pm.html#notes-from-excercises",
    "href": "20-10-25pm.html#notes-from-excercises",
    "title": "Lecture 8: Packaging and Dependency Management",
    "section": "Notes from excercises",
    "text": "Notes from excercises\n\nThe _ _init__.py gives access to functions which can be accessed with dot notation. In the following example, users can access func1 and func2:\n\n\n#parent_directory/__init__.py\nfrom .script1 import func1\nfrom .script2 import func2\n\n\nA common error can occur when calling a package from a different folder: ModuleNotFoundError. This is usually because the package directory is not in the system path. A quick fix to this is:\n\nimport sys \nsys.path.append('path/to/folder')\n\nA better solution is to make the directory into a package using setuptools. It can then be pip installed into the virtual environment.\n\n\nSetup tools, config files and pip\n\nIn order to create a package that can be pip installed, a pyproject.toml file is required. This contains some metadata about the package. An example is below:\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"] # setuptools and wheel are necessary for the build\n\n[project]\nname = \"tstools\"\nversion = \"0.1\"\ndescription = \"A package to analyse timeseries\"\nauthors = [\n    {name = \"Spam Eggs\", email = \"spam.eggs@email.com\"}\n]\nreadme = \"README.md\"\nlicense = {text = \"MIT\"}\ndependencies = [\"numpy\", \"matplotlib\", \"scipy\"]\n\n[project.urls]\nSource = \"example.com\"\n\n[project.scripts] # Define scripts here if you have any\n\n[project.optional-dependencies] # Define optional dependencies here if you have any\n\nThe folder structure should look like:\n\nmypackage_dist/\n\npyproject.toml\nmypackage/\n\ninit\nscripts\ntests\n\n\n\nNow can pip install path/to/pyproject.toml\nYou can pip install in editable mode, which means that changes in the package source code are automatically reflected by pip - you don’t need to reinstall after every change:\n\n# in directory to mypackage_dist/\npip install -e .\n\n\nBuild, wheels and source distribution\n\nThe build library allows you to easily prepare your project to be uploaded to PyPI:\n\npip install build wheel\npython -m build --sdist     # builds a sdist \npython -m build --wheel     # builds a wheel\npython -m build             # builds both\n\nsdists are sufficient for packages in pure python.\nwheels are used when there is precompiled code as well as pure python.\nBoth methods produce archive files (like zip and tar files), with the files in the original package plus some extra metadata.\nTo install a wheel\n\npython -m pip install packageroot/dist/packagename*.whl\n\nTwine is used to publish packages to PyPI.\n\npip install twine\ntwin upload path/to/.whl\n\nRepos can be uploaded to testpypi using\n\n# run from within packagename_dist folder so dist can be seen\ntwine upload --repository testpypi dist/*",
    "crumbs": [
      "Home",
      "Lecture 8: Packaging and Dependency Management"
    ]
  },
  {
    "objectID": "14-10-25-pm.html",
    "href": "14-10-25-pm.html",
    "title": "Lecture 3: Functional Programming",
    "section": "",
    "text": "Python is not a functional programming language so won’t cover in detail. It involves programming with pure functions and higher order functions.\n\nPure functions\nPure functions are referentially transparent - they always give the same output for the same inputs. It also means you could replace the entire function in the code with its output and it wouldn’t effect its result. e.g sqrt(4) is pure, but the function today() -&gt; 14/10/2025 is not a pure function as its return is different indepedent of the inputs. This also means a pure function cannot have any side effects.\nImpure things include: - changing the value of a variable - modifying a data structure in place - printing to the console - writing to a file - sending data over a network - reading from a file or keyboard - fetching data from a database or API - getting the current time or date - generating a random number\nThese are obviously useful. Most useful programmes are impure.\n\n\nWhy avoid impure functions\nMutablitility is the source of bugs. By limiting the things in your program that can be changed and the parts of it that can change them, bugs become easier to track.\nPure functions are easily testable because they give the same output for a given input. They can also be composed to create a new function. They are also easy to parallelise. e.g h(x) = g(x) + f(x), g and f can be run in parallel easily since the order in which they are run does not matter.\n\n\nDownsides of functional programming\nIf you never mutate anything, you require lots of copies of data which can be costly. Work arounds include: - using smart data structures that avoid copying where possible - using mutablility under the hood while providing functional style interface\npandas dataframes are an example of a smart data structure. It offers a functional interface, but uses non-functional features under the hood wnad won’t actually copy data unless it needs to.\npandas example:\n\ndata = {'A': [1,2,3], 'B':[4,5,6]}\ndf1 = pd.DataFrame(data)\ndf2 = df1.rename(columns = {'A' : 'a'})\n\nis functional because it returns df2 separately as a new dataframe without changing df1. Under the hood, however, data is changed for optimisation reasons\n\n\nNotable functional languages\n\nLisp\nHaskell\nScala\nF#\nGoogle Earth Engine?\n\nFirst-class functions are functions that can be passed as arguments, returned from functions, or assigned to variables.\nLambda functions are small, unnamed functions defined in the normal flow of the program. They are typically defined at the point of use, unlike usual functions\ne.g\n\n# first class functions\ndef greed():\n    return 'Hello'\n\nhello = greet\nprint(hello())\n\n\n# lambda functions\nadd = lambda x, y: x + y\nprint(add(2, 3))\n\nHigher order functions are functions that take other arguments as functions e.g map, filter, reduce functions in pythons. These use lazy evaluation - when you call map, it returns an object (generator), but does not actually calculate the map until you for it. In the example below, the list() call executes the generator. Lazy execution is used so that different functions can be stacked to create complex generators, then computed all at once. This means the input only needs to be looped over once.\n\nnumbers = [1,2,3,4]\n\n# map\nsquared = list(map(lambda x: x**2, numbers))\nprint(squared) \n# output [1, 4, 9, 16, 25]\n\n\n# reduce\nfrom functools import reduce\nproduct = reduce((lambda x, y: x * y), numbers)\nprint(product)\n# output 120\n\nNote that sum, mean, stdev, max, min, len etc are examples of the reduce function\nAny time you have a collection of things and you need to transform it into a new collection of things, consider using higher order functions (map, filter, reduce) rather than using loops. This allows parallelisation as the functions are pure, clearer code, and less likely to create bugs.\n\n\nList comprehensions and generators\nMore pythonic way to do maps. Generators are similar to list comprehensions but are lazily executed so won’t generate new list unless asked to, useful for composition.\n\nsquared = [x**2 for x in numbers]\n\nsquared_dict = {x: x**2 for x in numbers}\n# output {1:1, 2:4, 3:9, 4:16, 5:25}\n\nsquared_gen = (x**2 for x in numbers)\nfor num in squared_gen:\n    print(num)\n\n\n\nDecorators\nHigher order functions that take another function as an argument, modifies it, then returns it.\nExample use case: have a code that depends on a database and you want to test the code with some test data. You could use a decorator that sets up database first and adds the test data, calls test function (which is already defined), then cleans up after itself, deleting the test data added.\nSimple example:\n\ndef my_decorator(func):\n    def wrapper():     \n        print('before func call')\n        func()\n        print('after func call')\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print('Hello')\n\n#&gt;say_hello()\n#before function call\n#Hello\n#After function \n\nSo modifies say_hello() to include print statements - in this way we can modify functions to have things happening before and after\n\n\nRecursion\nCalling a function from within itself\n\n\nCode\n\n\n# Recursive\ndef factorial(n):\n    if n==0:\n        return 1\n    return n * factorial(n-1)\n\nThis works because of the mathematical definition of recursion \\[\n\\begin{align}\n&0! = 1 \\\\\n&n! = n \\times (n-1)!\n\\end{align}\n\\]\n\n\n\nSummary\nFunctional programming improves - testability - composability - parallelisability\nIn functional programming, we shift from an imperative mindset (‘do this, do that, do this’) to a declarative one (‘here is what I need’) and it becomes about chaining transformation to the data.",
    "crumbs": [
      "Home",
      "Lecture 3: Functional Programming"
    ]
  },
  {
    "objectID": "16-10-25am.html",
    "href": "16-10-25am.html",
    "title": "My Lecture Notes",
    "section": "",
    "text": "A tool that tracks changes to raw text files.\nRecords changes you made and the order in which you made them.\nOffers more efficient backup.\nEasy to roll back (all of part of) your work.\nCan share one version with collaborators whilst you work on another.\nDon’t add result/output files to version control - think about this when setting up file storage.\nStore code in separate files where possible. This allows two people to work on separate files, making merge conflicts less likely.\nNever store sensitive information in version control\n\n\n\n\n\nStart by storing the original base version of the file.\nAfter that, only changes are stored.\nTwo users make independent sets of changes, creating two different versions of the same document (called branches).\nCan merge different branches together onto the same base document.\n\n\n\n\n\nServices offered by companies which provide remote repositories to store your files. This protects files from fire, theft, etc.\nSeparate but related to the version control software itself (git).\nGithub is most popular. Gitlab also popular, but falling out of favour. Offers more control than github, but more difficult to use.\nGithub student accounts offer free perks.\nTo set up git and github - generate ‘ssh key pair’ to authenticate yout computer - essentially replaces passwords\n\none private key that never leaves your computer\none public key\n\nAnyone with the public key can verify that you own the private key without them having to see\nUpload the public one to Github as authentication\nNow can securely transfer\n\n\n\n\n\nDifferent groups have different ways of working with version control\nA workflow is agreed upon e.g have a main branch and a development branch that people contribute to before adding to main branch.\n\n\n\n\n\nBest way to use git for learning\nNecessary for HPC clusters\nOnce comfortable, use as part of IDE graphically",
    "crumbs": [
      "Home",
      "Lecture 4: Version Control with Git and Github"
    ]
  },
  {
    "objectID": "16-10-25am.html#lecture-4-version-control-with-git-and-github",
    "href": "16-10-25am.html#lecture-4-version-control-with-git-and-github",
    "title": "My Lecture Notes",
    "section": "",
    "text": "A tool that tracks changes to raw text files.\nRecords changes you made and the order in which you made them.\nOffers more efficient backup.\nEasy to roll back (all of part of) your work.\nCan share one version with collaborators whilst you work on another.\nDon’t add result/output files to version control - think about this when setting up file storage.\nStore code in separate files where possible. This allows two people to work on separate files, making merge conflicts less likely.\nNever store sensitive information in version control\n\n\n\n\n\nStart by storing the original base version of the file.\nAfter that, only changes are stored.\nTwo users make independent sets of changes, creating two different versions of the same document (called branches).\nCan merge different branches together onto the same base document.\n\n\n\n\n\nServices offered by companies which provide remote repositories to store your files. This protects files from fire, theft, etc.\nSeparate but related to the version control software itself (git).\nGithub is most popular. Gitlab also popular, but falling out of favour. Offers more control than github, but more difficult to use.\nGithub student accounts offer free perks.\nTo set up git and github - generate ‘ssh key pair’ to authenticate yout computer - essentially replaces passwords\n\none private key that never leaves your computer\none public key\n\nAnyone with the public key can verify that you own the private key without them having to see\nUpload the public one to Github as authentication\nNow can securely transfer\n\n\n\n\n\nDifferent groups have different ways of working with version control\nA workflow is agreed upon e.g have a main branch and a development branch that people contribute to before adding to main branch.\n\n\n\n\n\nBest way to use git for learning\nNecessary for HPC clusters\nOnce comfortable, use as part of IDE graphically",
    "crumbs": [
      "Home",
      "Lecture 4: Version Control with Git and Github"
    ]
  },
  {
    "objectID": "16-10-25am.html#notes-from-exercises",
    "href": "16-10-25am.html#notes-from-exercises",
    "title": "My Lecture Notes",
    "section": "Notes from exercises",
    "text": "Notes from exercises\n\nSetting up Git\nWhen on a new machine, need to configure username, email, and default text editor to use when adding commit messages etc.\ngit config --global user.name \"Firstname Surname\"\ngit config --global user.email \"fsurname@university.ac.uk\"\ngit config --global core.editor \"nano -w\"\nThe configuration settings can be view with\ngit config --list\nHelp can be accessed with\ngit config -h       # and\ngit config --help\nTo back work up to GitHub, a pair of SSH keys is needed. To generate these, type\nssh-keygen -t ed25519\nThis creates a folder in the root directory called /.ssh. Inside, there is a private and public key - the public ends with extension .pub. This can be printed in the terminal with\ncat ~/.ssh/id_ed25519.pub\nThis can be copied into github’s SSH settings (copy the entire output of cat).\n\n\nCloning repositories\n\nFind the desired repo on Github, click on the Code box, then clone. Select SSH and copy the address given.\nIn the terminal, type\n\ngit clone {repository url}\n\n\nInitialising your own repository\n\nEnter an existing repo and type ‘git init’. This turns it into a git repo, which can be verified with ‘ls -a’ to see a .git folder.\nNext, make an empty repo on GitHub and name it. Then copy the SSH url option and type\n\ngit remote add origin {ssh url}\n\nIf the repo already contains files, they need to be added and committed. This can be done with ‘git add .’, then git commit -m ‘your commit message’. ‘git add .’ adds all files that have been changed (in this case every file in the repo).\nOnce the commits have been made, they can be pushed to github with\n\ngit push -u origin main\nThe origin main connects the local main branch with the main branch of the remote repo. The -u saves this as the default connection for all future pushes.\n\n\nWorkflow\n\nThe general workflow in Git is:\n\nmake a change to a file\nadd all desired changes to the staging area\ncommit all changes in staging area (saves these changes)\npush these changes to the remote branch\n\nUseful commands include:\n\ngit status: shows the changes in the staging area to be committed\ngit diff: shows an overview of the changes made to a file. Can view changes compared with versions saved in older commits using git diff HEAD~n filename, where n is the number of commits prior you want to check. Alternatively, you can use the identifier of the commit you want to compare with (obtained from git log)\ngit log: shows the history of the repo (previous commits and pushes etc)\ngit restore: returns the previous state of a file (obtained from last commit). git checkout\n\nfilename can be used to restore a file from any previously saved commit.\n\n\n\n\nUsing the remote branch\n\nIf a local branch is already connected with the corresponding remote branch (e.g main local is connected with main remote), then the command used is\n\ngit push\n\nIf it is the first time pushing to a new branch\n\ngit push -u origin {desired name of remote branch}\nThe -u sets this behaviour as default for this branch.\n\nA push can fail if a conflict between the local and remote branches occurs. This typically happens if the same lines of the same file is edited both on the remote branch and on the local. To fix this, we use ‘git pull’ to pull the new file, fix the conflicts on our own pc, then push again once they have been resolved. Note when git pull is used for the first time, a default method for merging may be required. In this case, the following and try ‘git pull’ again\n\ngit config --global pull.rebase false\n\nTo mitigate conflicts, code should be split into multiple files where possible, and separate branches be used.\n\n\n\nBranches\n\nTo view current branches, type\n\ngit branch\nThe current branch is indicated with an *.\n\nTo create a new branch, type\n\ngit branch {new branch name}\n\nTo switch branches type\n\ngit switch {branch name}\n\nTo merge branches from branch a to branch b, navigate to branch a and type\n\ngit merge {branch name}\nThis can be followed by git push to update the remote branch as usual.\n\nFor larger projects, its best to use a pull request to merge branches. When a new push has been made to a branch, Github displays a banner saying ‘Compare and Pull Request’. This does a similar thing to git merge, but gives a graphical UI that allows notes to be made, requests for senior colleagues to check over work, and more.\n\n\n\nIgnoring files\n\nTo stop git tracking certain files, a file called .gitignore can be made in the root directory of the project.\nThe contents of the file may look like\n\n*.dat\nresults/\nIn this case, all files ending .dat, and all files within the results folder, will not be tracked by git.\n\nThis is useful as typically data files and other output files should not be tracked using git as it may take up lots of disk space, and distracts from the relevent details.\nEmpty folder are not pushed to git, e.g the results folder if all contents have been added to .gitignore, the folder will not appear in the remote branch. To fix this, a file called .gitkeep can be added to folder. We then add this to the commit using\n\ngit add -f folder/.gitkeep\nNote that the -f (force) is required to overide the .gitignore instructions. This can be commited and pushed. The folder should now appear in the remote branch",
    "crumbs": [
      "Home",
      "Lecture 4: Version Control with Git and Github"
    ]
  }
]